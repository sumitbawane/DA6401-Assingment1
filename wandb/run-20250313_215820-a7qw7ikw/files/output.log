Initializing network with layer sizes: [784, 128, 128, 128, 128, 128, 10]
Using activation function: relu
Using initialization method: xavier
Layer 0: Weight shape = (784, 128), Bias shape = (128,)
Layer 1: Weight shape = (128, 128), Bias shape = (128,)
Layer 2: Weight shape = (128, 128), Bias shape = (128,)
Layer 3: Weight shape = (128, 128), Bias shape = (128,)
Layer 4: Weight shape = (128, 128), Bias shape = (128,)
Layer 5: Weight shape = (128, 10), Bias shape = (10,)
Original y shape before one-hot encoding: (54000,)
One-hot encoded y shape: (54000, 10)
Original y shape before one-hot encoding: (6000,)
One-hot encoded y shape: (6000, 10)
Original y shape before one-hot encoding: (10000,)
One-hot encoded y shape: (10000, 10)
Epoch 1/5, Train Loss: 2.3026, Train Acc: 0.1009, Val Loss: 2.3028, Val Acc: 0.0917
Epoch 2/5, Train Loss: 2.3026, Train Acc: 0.0999, Val Loss: 2.3028, Val Acc: 0.1013
Epoch 3/5, Train Loss: 2.3026, Train Acc: 0.1009, Val Loss: 2.3027, Val Acc: 0.0917
Epoch 4/5, Train Loss: 2.3026, Train Acc: 0.1009, Val Loss: 2.3028, Val Acc: 0.0917
Epoch 5/5, Train Loss: 2.3027, Train Acc: 0.1009, Val Loss: 2.3029, Val Acc: 0.0917
Test Loss: 2.3027, Test Accuracy: 0.1000
